import requests
import pandas as pd
import xlsxwriter 
from random import choice as random_choice
from time import sleep as time_sleep
from bs4 import BeautifulSoup
from numpy import nan as np_nan

def getHTMLText(url, code = 'utf-8', num_retries = 2, proxies = {}, sleep = 0, params = None):
    '''
    获取HTML的Text文本内容
    '''
    time_sleep(sleep)
    headers = {'user-agent':'Mozilla/5.0'}
    try:
        r = requests.get(url, headers = headers, proxies = {}, params = params)   
        r.raise_for_status()
        r.encoding = code
        print('HTML Text Ready! Status_code = {}  Len(target_text) = {}'.format(r.status_code, len(r.text)))
        return r.text
    except requests.HTTPError as e:
        print('Download error:', e)
        if num_retries > 0:
            if 500 <= r.status_code < 600:
                num_retries -= 1
                print('Retring!')
                return getHTMLText(url, num_retries = num_retries, proxies = proxies, sleep = sleep, params = params)
    
def getHTMLText_proxy(url, use_proxies = False, proxytype = 'http', proxies_list = [], code = 'utf-8', sleep = 0, params = None):
    '''
    使用代理ip获取HTML的Text文本内容
    '''
    time_sleep(sleep)
    if use_proxies == False:
        r = getHTMLText(url,code = code, sleep = sleep)
        return r
    if proxies_list == []:
        r = getHTMLText(url,code = code, sleep = sleep)
        return r
    if proxytype == 'http':
        proxies_list = [i['http'] for i in proxies_list]
    if proxytype == 'https':
        proxies_list = [i['https'] for i in proxies_list]

    if url[:5].lower() == 'https':
        if proxytype != 'https':
            print('Https proxies not found!',end = ' ')
            print('Downloading:',url)
            r = getHTMLText(url, code = code, sleep = sleep, params = params)
        else:
            random_proxies = {'https': random_choice(proxies_list)}
            print("Downloading '{}' with proxies {} !".format(url, random_proxies))
            try:
                r = getHTMLText(url,proxies = random_proxies,code = code, sleep = sleep, params = params)
            except:
                random_proxies = {'https': random_choice(proxies_list)}
                r = getHTMLText(url,proxies = random_proxies,code = code, sleep = sleep, params = params)
    elif url[:4].lower() == 'http':
        if proxytype != 'http':
            print('Http proxies not found!',end = ' ')
            print('Downloading:',url)
            r = getHTMLText(url, code = code, sleep = sleep, params = params)
        else:
            random_proxies = {'http': random_choice(proxies_list)}
            print("Downloading '{}' with proxies {} !".format(url, random_proxies))
            try:
                r = getHTMLText(url,proxies = random_proxies,code = code, sleep = sleep, params = params)
            except:
                random_proxies = {'https': random_choice(proxies_list)}
                r = getHTMLText(url,proxies = random_proxies,code = code, sleep = sleep, params = params)
    return r  

def getProxyList(num = 20, v_num = None, head = None, iptype = None, loc = None):
    '''
    获取代理ip列表
    name	type	Description	Optional	example	Remarks
    num	int	IP 数量	可选	10	每次最多20
    v_num	int	验证通过次数	可选	5	通过次数越多，IP越稳定
    type	str	ip类型	可选	O	G-'高匿',T-'透明',O-'其他'
    head	str	http 或者 https	可选	https	默认为 http
    loc	str	地区	可选	上海	尽量以省市一级的地名查询
    '''
    proxyURL = r"http://lab.crossincode.com/proxy/get/"
    contents = {}
    contents['num'] = num
    contents['v_num'] = v_num
    contents['head'] = head
    contents['iptype'] = iptype
    contents['loc'] = loc
    html = requests.get(proxyURL,params = contents)
    iplist = html.json()
    iplist = iplist['proxies']
    return iplist     

'''
以上代码用于获取网页文本数据，为通用代码，一般不需修改！
'''

def parse_infolist(target_text, infolist=[], keylist = ['基金','金融','银行','保险','投资']):
    '''解析页面信息列表'''
    soup = BeautifulSoup(target_text, 'html.parser')
    try:
        ul = soup.find_all('ul', attrs={'class':'sojob-list'})[0]
        tr = ul.find_all('li')
    except:
        print(target_text)
    count = 0
    for value in tr:
        try:
            try:
                leixing = value.find_all('p', attrs={'class':'field-financing'})[0].a.get_text().replace(' ','')
            except:
                leixing = '-'
            try:
                gongsi = value.find_all('p', attrs={'class':'company-name'})[0].a.get_text().replace(' ','')
            except:
                gongsi = '-'
            
            '''筛选重点字段'''
            targettype = '是' if any(keyword in str(leixing) for keyword in keylist) else '否'
            if any(keyword in str(gongsi) for keyword in keylist):
                targettype = '是'

            url = value.h3.a.get('href')
            title = value.h3.get('title')[2:]
           
            try:
                xinzi = value.find_all('span', attrs={'class':'text-warning'})[0].get_text()
            except:
                xinzi = '-'
            try:
                area = value.find_all('a', attrs={'class':'area'})[0].get_text()
            except:
                area = '-'
            try:
                edu = value.find_all('span', attrs={'class':'edu'})[0].get_text()
            except:
                edu = '-'
            try:
                nianxian =value.find_all('span')[2].get_text()
            except:
                nianxian = '-'
            try:
                update = value.find_all('time')[0].get_text().replace(' ','')
            except:
                update = '-'
            try:
                fankui = value.find_all('p', attrs={'class':'time-info clearfix'})[0].span.get_text().replace(' ','')
            except:
                fankui = '-'
            try:
                fuli = value.find_all('p', attrs={'class':'temptation clearfix'})[0].text.replace('\n','')
            except:
                fuli = '-'
            if 'https' not in str(url):
                url ='https://www.liepin.com{}'.format(str(url))          
            infolist.append([title, xinzi, area, edu, nianxian, update, fankui, gongsi, leixing, targettype, fuli, url])    
        except:
            print(count,title,xinzi,area,gongsi,leixing,targettype,url)
            continue
        count +=1
    return infolist

def parse_info(nexturl, maxpage = 1, infolist = [], headers = {'user-agent':'Mozilla/5.0'}, proxytype = 'http', proxies_list = [], cookies = [], sleep = 1, keylist = []):
    
    curPage = 1
    d_curPage = 0
    for page in range(1,3):
#    for page in range(1,int(maxpage) + 1):
        print("数据共{}页，正在获取第{}页信息……".format(int(maxpage) + 1, int(page) + 1))
        pageurl = r"{}{}&curPage={}".format(nexturl[:-11], d_curPage, curPage)
        target_text = getHTMLText(pageurl)
        time_sleep(1)
        infolist = parse_infolist(target_text, infolist=infolist, keylist = keylist)
        curPage += 1
        d_curPage += 1        
    dflist = pd.DataFrame(infolist)
    dflist.columns = ['标题','薪资','位置','教育程度','工作年限','更新时间','反馈时间','公司名称','公司类型','重点关注','福利','网址']
    return dflist

def parse_page(df, headers = {'user-agent':'Mozilla/5.0'}, proxytype = 'http', proxies_list = [], cookies = [], sleep = 1):
    df['描述']= 0
    for page in range(0,2):
    #for page in range(0,len(df.index)):
        print("数据共{}页，正在获取第{}页信息……".format(len(df.index) + 1, int(page) + 1))
        pageurl = df.iloc[page, 11]
        target_text = getHTMLText_proxy(pageurl, proxytype = proxytype, proxies_list = proxies_list )
        time_sleep(sleep)
        soup = BeautifulSoup(target_text, 'html.parser')
        try:
            miaoshu = soup.find_all('div',attrs={'class':'content content-word'})[0].get_text().replace(' ','').replace('\n','')
        except:
            miaoshu = soup.find_all('div',attrs={'class':'job-info-content'})[0].get_text().replace(' ','').replace('\n','')
        df.iloc[page,12] = miaoshu
    return df

def to_excel(df, location):
    nrows = len(df.index)
    ncols = len(df.columns)
    workbook = xlsxwriter.Workbook(location)
    top = workbook.add_format({'border':1,'align':'center','font_size':12,'bold':True})
    zhongdian = workbook.add_format({'border':1, 'bg_color':'fff68f', 'bold':True})
    feizhongdian = workbook.add_format({'border':1})
    worksheet = workbook.add_worksheet(name='数据列表')
    worksheet.set_column(0,0,25)
    worksheet.set_column(7,7,15)
    for i in range(nrows):
        #worksheet.set_row(i,22)                 #设定第i行单元格属性，高度为22像素，行索引从0开始
        for j in range(ncols):
            
            cell_value = df.iloc[i,j] #获取第i行中第j列的值
            if cell_value is np_nan:
                cell_value = '-'
            
            if i == 0:
                format = top
                format.set_text_wrap()
                format.set_align('vcenter')                 #设置单元格垂直对齐
                worksheet.write(i,j,df.columns[j],format)
            elif str(df.iloc[i,9]) == '是':
                format = zhongdian
                worksheet.write(i,j,cell_value,format)      #把获取到的值写入文件对应的行列
            else:
                format = feizhongdian
                worksheet.write(i,j,cell_value,format)      #把获取到的值写入文件对应的行列
    worksheet.freeze_panes(1, 0)  # Freeze the first row.        
    workbook.close()

class liepin(object):
    
    def __init__(self,params = None):

        self.main = r"https://www.liepin.com/zhaopin/"
        _names = ""
        for _k,_v in params.items():
            if _k not in ['dqs','salary']:
                _names += '-{}'.format(_v)
        self.save = r"D:\猎聘{}.xlsx".format(_names)
        self.text = requests.get(self.main, params = params).text
        self.soup = BeautifulSoup(self.text, 'html.parser')
        self.maxpage = self.soup.find_all('a', attrs={'class':'last'})[0].get('href')[-2:]
        pagenum = self.soup.find_all('div', attrs={'class':'pagerbar'})[0].find_all('a')
        nextpage = [ipage.get('href') for ipage in pagenum if ipage.text =="下一页"][0]
        self.nexturl = r'https://www.liepin.com{}'.format(nextpage)


if __name__ == "__main__":

    '''目前查询条件：{地址dps:北京010,工资salary:30-60,关键词key:营销策划}'''    
    params = {'dqs':'010',
              'salary':'30$60',
              'key':'产品经理'}
    zhongdianlist = ['基金','金融','银行','保险','投资']
    
    proxies_list = getProxyList(20, head = 'https')
    lp = liepin(params)
    infolist = parse_infolist(lp.text, keylist = zhongdianlist)
    dflist = parse_info(lp.nexturl, lp.maxpage, infolist, sleep = 1, keylist = zhongdianlist)
    df_final = dflist.drop_duplicates(subset = ['公司名称','标题','薪资','网址'])
    df_zd = df_final.loc[df_final['重点关注'] == '是']
    df_zd = parse_page(df_zd, proxytype = 'https' , proxies_list = proxies_list ,sleep = 1)
    df_finally = pd.merge(df_final,df_zd,how='left')
    to_excel(df_finally,lp.save)
    
